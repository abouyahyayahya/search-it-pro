# -*- coding: utf-8 -*-
"""
Search It â€” Pro (i18n Actions)
------------------------------
- ØªØ±Ø¬Ù…Ø© ÙƒÙ„ Ø§Ù„Ø£Ø²Ø±Ø§Ø±/Ø§Ù„Ø£ÙŠÙ‚ÙˆÙ†Ø§Øª Ø¥Ù„Ù‰ 10 Ù„ØºØ§Øª (ØªØªØ¨Ø¯Ù„ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠÙ‹Ø§ Ù…Ø¹ Ø§Ø®ØªÙŠØ§Ø± Ù„ØºØ© Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©).
- Ø±Ø¨Ø· Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙŠÙ‚ÙˆÙ†Ø§Øª Ø§Ù„Ø®Ø¯Ù…ÙŠØ© Ø¨Ø±Ø§Ø¨Ø· ÙˆØ§ØªØ³Ø§Ø¨ Ù…ÙˆØ­Ù‘Ø¯: 00201007975534
- ØªØ­Ø³ÙŠÙ† Ø¨Ù†Ø§Ø¡ Ù†Øµ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨Ø­Ø³Ø¨ Ù„ØºØ© Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ÙˆØ¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¹Ù…Ù„.
"""

import io
import os
import re
import json
import requests
import pandas as pd
import streamlit as st
from urllib.parse import quote

# ---------------- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ù‘Ø© ----------------
LANGS = [
    ("ar","Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"),
    ("fa_AF","Ø§Ù„Ø£ÙØºØ§Ù†ÙŠØ© (Ø¯Ø§Ø±ÙŠ/Ø¨Ø´ØªÙˆ)"),
    ("en","English"),
    ("fr","FranÃ§ais"),
    ("de","Deutsch"),
    ("ru","Ğ ÑƒÑÑĞºĞ¸Ğ¹"),
    ("zh","ä¸­æ–‡"),
    ("es","EspaÃ±ol"),
    ("tr","TÃ¼rkÃ§e"),
    ("fa","ÙØ§Ø±Ø³ÛŒ"),
]

DEFAULT_CONFIG = {
    "brand_title": "Search It â€” Pro",
    "openalex_base": "https://api.openalex.org/works",
    "openalex_mailto": "",
    "ui": {"lang": "ar"},
    "filters": {"years_min": 1990, "years_max": 2030, "open_access_only": False}
}

def _merge_dicts(base, other):
    out = dict(base)
    for k, v in other.items():
        if isinstance(v, dict) and k in out and isinstance(out[k], dict):
            out[k] = _merge_dicts(out[k], v)
        else:
            out[k] = v
    return out

def load_config():
    for name in ("config_search_it.json", "config_searche_today.json"):
        p = os.path.join(os.getcwd(), name)
        if os.path.exists(p):
            try:
                with open(p, "r", encoding="utf-8") as f:
                    user_cfg = json.load(f)
                    return _merge_dicts(DEFAULT_CONFIG, user_cfg)
            except Exception:
                pass
    return DEFAULT_CONFIG

CFG = load_config()
st.set_page_config(page_title=CFG.get("brand_title","Search It â€” Pro"), page_icon="ğŸ”", layout="wide")

# ---------------- ØªØ±Ø¬Ù…Ø§Øª ----------------
# Ù…ÙØ§ØªÙŠØ­ Ù‚Ø§Ø¨Ù„Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ÙˆØ§Ù„Ø£Ø²Ø±Ø§Ø±
T = {
    "title": {k: "Search It â€” Pro" for k,_ in LANGS},
    "search_placeholder": {
        "ar": "Ø§ÙƒØªØ¨ ÙƒÙ„Ù…ØªÙƒ Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©â€¦",
        "en": "Enter your keywordâ€¦",
        "fr": "Entrez votre mot-clÃ©â€¦",
        "de": "Suchbegriff eingebenâ€¦",
        "ru": "Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾â€¦",
        "zh": "è¾“å…¥å…³é”®è¯â€¦",
        "es": "Escribe tu palabra claveâ€¦",
        "tr": "Anahtar kelime girinâ€¦",
        "fa": "Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯â€¦",
        "fa_AF": "Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯â€¦",
    },
    "search_button": {k: "ğŸ” Search" for k,_ in LANGS},
    "adv_search": {k: "Advanced search" for k,_ in LANGS},
    "total_results": {k: "Total results (upstream)" for k,_ in LANGS},
    "displayed_after_filter": {k: "Displayed after local filtering" for k,_ in LANGS},
    # ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ø£ÙŠÙ‚ÙˆÙ†Ø§Øª Ø§Ù„Ø®Ø¯Ù…ÙŠØ©
    "icon_download_pdf": {k: "â¬‡ï¸ Download (PDF)" for k,_ in LANGS},
    "icon_download_source": {k: "â¬‡ï¸ Download from source" for k,_ in LANGS},
    "icon_stats_q": {k: "ğŸ“Š I have a question about research statistics" for k,_ in LANGS},
    "icon_method_q": {k: "ğŸ§ª I have a question about methodology & results" for k,_ in LANGS},
    "icon_analysis_service": {k: "ğŸ§® I want statistical analysis service" for k,_ in LANGS},
    "icon_whatsapp": {k: "ğŸ“ Contact us for a research consultation (WhatsApp)" for k,_ in LANGS},
    # Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§
    "meta_venue": {k: "Journal" for k,_ in LANGS},
    "meta_authors": {k: "Authors" for k,_ in LANGS},
    "meta_year": {k: "Year" for k,_ in LANGS},
    "meta_citations": {k: "Citations" for k,_ in LANGS},
}
# Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
T["search_button"]["ar"] = "ğŸ” Ø§Ø¨Ø­Ø«"
T["adv_search"]["ar"] = "Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"
T["total_results"]["ar"] = "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±)"
T["displayed_after_filter"]["ar"] = "Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø© Ø¨Ø¹Ø¯ Ø§Ù„ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©"
T["icon_download_pdf"]["ar"] = "â¬‡ï¸ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± (PDF)"
T["icon_download_source"]["ar"] = "â¬‡ï¸ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±"
T["icon_stats_q"]["ar"] = "ğŸ“Š Ù„Ø¯ÙŠ Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ø¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„Ø¨Ø­Ø«"
T["icon_method_q"]["ar"] = "ğŸ§ª Ù„Ø¯ÙŠ Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„Ø¨Ø­Ø« ÙˆÙ†ØªØ§Ø¦Ø¬Ù‡Ø§"
T["icon_analysis_service"]["ar"] = "ğŸ§® Ø£Ø±ØºØ¨ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ"
T["icon_whatsapp"]["ar"] = "ğŸ“ ØªÙˆØ§ØµÙ„ Ù…Ø¹Ù†Ø§ Ù„Ø·Ù„Ø¨ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø¨Ø­Ø«ÙŠØ© (ÙˆØ§ØªØ³Ø§Ø¨)"
T["meta_venue"]["ar"] = "Ø§Ø³Ù… Ø§Ù„Ù…Ø¬Ù„Ø©"
T["meta_authors"]["ar"] = "Ø§Ù„Ù…Ø¤Ù„ÙÙˆÙ†"
T["meta_year"]["ar"] = "Ø§Ù„Ø¹Ø§Ù…"
T["meta_citations"]["ar"] = "Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª"

def t(key, lang):
    return T.get(key, {}).get(lang, T.get(key, {}).get("en", key))

# ---------------- Ø­Ø§Ù„Ø§Øª Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ----------------
if "ui_lang" not in st.session_state:
    st.session_state.ui_lang = CFG["ui"]["lang"]

# ---------------- Ø±Ø£Ø³ Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù„ØºØ© ----------------
top_left, top_center, top_right = st.columns([1,2,1])
with top_center:
    st.markdown(f"<h2 style='text-align:center'>{t('title', st.session_state.ui_lang)}</h2>", unsafe_allow_html=True)
with top_right:
    st.session_state.ui_lang = st.selectbox("Language", [code for code,_ in LANGS],
        index=[code for code,_ in LANGS].index(st.session_state.ui_lang),
        format_func=lambda code: next((lbl for c,lbl in LANGS if c==code), code))

# ---------------- Ø´Ø±ÙŠØ· Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙˆØ³Ø· ----------------
st.markdown("""
<style>
.center-box {max-width: 900px; margin: 0 auto; padding: 0.5rem 1rem; border-radius: 16px; background: #f8f9fb;}
</style>
""", unsafe_allow_html=True)

with st.container():
    st.markdown("<div class='center-box'>", unsafe_allow_html=True)
    q = st.text_input("", key="q_main", placeholder=t("search_placeholder", st.session_state.ui_lang), label_visibility="collapsed")
    cols = st.columns([6,2])
    with cols[0]:
        st.markdown("&nbsp;", unsafe_allow_html=True)
    with cols[1]:
        do_search = st.button(t("search_button", st.session_state.ui_lang))
    st.markdown("</div>", unsafe_allow_html=True)

# ---------------- Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ----------------
with st.expander(t("adv_search", st.session_state.ui_lang), expanded=False):
    c1, c2, c3 = st.columns(3)
    with c1:
        exact_phrase = st.text_input("Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©", value="")
        any_words    = st.text_input("Ø£ÙŠÙ‘ÙŒ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª (Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„)", value="", placeholder="ØªØ¹Ù„Ù… Ø¢Ù„Ø©, Ø±Ø¤ÙŠØ© Ø­Ø§Ø³ÙˆØ¨ÙŠØ©")
        none_words   = st.text_input("Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª", value="", placeholder="Ø§Ø³ØªØ¹Ø±Ø§Ø¶, Ù…Ø±Ø§Ø¬Ø¹Ø©")
        lang_code    = st.selectbox("Ù„ØºØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬", [c for c,_ in LANGS], index=[c for c,_ in LANGS].index(st.session_state.ui_lang))
    with c2:
        author       = st.text_input("Ø§Ø³Ù… Ø§Ù„Ù…Ø¤Ù„Ù", value="", placeholder="Ù…Ø«Ø§Ù„: Andrew Ng")
        venue        = st.text_input("Ø§Ù„Ù…Ø¬Ù„Ø©/Ø§Ù„Ù…Ø¤ØªÙ…Ø±", value="", placeholder="Ù…Ø«Ø§Ù„: NeurIPS")
        doc_type     = st.multiselect("Ù†ÙˆØ¹ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©", ["article","proceedings-article","book","monograph","dataset","dissertation","report","other"], default=[])
        open_access_only = st.checkbox("Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ù…ÙØªÙˆØ­ ÙÙ‚Ø·", value=False)
    with c3:
        years        = st.slider("Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø²Ù…Ù†ÙŠ", 1990, 2030, (1990, 2030))
        sort_opt     = st.selectbox("Ø§Ù„ØªØ±ØªÙŠØ¨", ["Ø§Ù„ØµÙ„Ø© (Ø§ÙØªØ±Ø§Ø¶ÙŠ)","Ø§Ù„Ø£Ø­Ø¯Ø«","Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ù‹Ø§"], index=0)
        title_only   = st.checkbox("ğŸ” Ø­ØµØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† (title.search)", value=False)
        strict_local = st.checkbox("ğŸ§² Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠ (ØªØ·Ø§Ø¨Ù‚ Ø°ÙƒÙŠ)", value=True)

# ---------------- Ø£Ø¯ÙˆØ§Øª Ø·Ù„Ø¨ OpenAlex ----------------
def _ok(s): 
    return bool(re.search(r'[A-Za-z0-9\u0600-\u06FF]', (s or "")))

def _field(obj, path, default=None):
    cur = obj
    for p in path.split("."):
        if isinstance(cur, dict) and p in cur:
            cur = cur[p]
        else:
            return default
    return cur

def _reconstruct_abstract(inv_index):
    if not inv_index or not isinstance(inv_index, dict):
        return ""
    pos_to_word = {}
    max_pos = -1
    for word, positions in inv_index.items():
        for p in positions:
            pos_to_word[p] = word
            max_pos = max(max_pos, p)
    return " ".join(pos_to_word.get(i, "") for i in range(max_pos+1)).strip() if max_pos >= 0 else ""

def _tokenize(text):
    if not text: return []
    return [t for t in re.split(r'[\s,;:ØŒØ›]+', text.strip()) if t]

def _apply_exclude(items, none_words):
    if not none_words: 
        return items
    excl = [x.strip().lower() for x in re.split(r'[,\s]+', none_words) if x.strip()]
    if not excl:
        return items
    filtered = []
    for w in items:
        title = (w.get("title") or "").lower()
        abstract = _reconstruct_abstract(_field(w, "abstract_inverted_index") or {}).lower()
        if any(x and ((x in title) or (x in abstract)) for x in excl):
            continue
        filtered.append(w)
    return filtered

def _apply_require(items, q, exact_phrase, any_words, enabled=True):
    if not enabled:
        return items
    req_tokens = []
    if q: req_tokens.extend(_tokenize(q))
    if exact_phrase: req_tokens.append(exact_phrase.strip())
    any_list = [w.strip() for w in any_words.split(",") if w.strip()] if any_words else []

    out = []
    for w in items:
        text = ((w.get("title") or "") + " " +
                _reconstruct_abstract(_field(w, "abstract_inverted_index") or ""))
        t = text.lower()
        ok_all = all(tok.lower() in t for tok in req_tokens) if req_tokens else True
        ok_any = any((aw.lower() in t) for aw in any_list) if any_list else True
        if ok_all and ok_any:
            out.append(w)
    return out

def _build_params(q, exact_phrase, any_words, author, venue, lang_code, years, open_access_only, doc_type, sort_opt, title_only):
    params, search_terms = {}, []
    if _ok(q):             search_terms.append(q.strip())
    if _ok(exact_phrase):  search_terms.append(f"\"{exact_phrase.strip()}\"")
    if any_words:
        terms = [w.strip() for w in any_words.split(",") if _ok(w)]
        if terms: search_terms.append(" OR ".join(terms))
    if search_terms:
        params["search"] = " ".join(search_terms)

    filters = []
    if _ok(author): filters.append(f"authorships.author.display_name.search:{author.strip()}")
    if _ok(venue):  filters.append(f"host_venue.display_name.search:{venue.strip()}")
    if lang_code:   filters.append(f"language:{lang_code}")
    if years and isinstance(years, (list, tuple)) and len(years) == 2:
        y0, y1 = years
        filters.append(f"from_publication_date:{y0}-01-01")
        filters.append(f"to_publication_date:{y1}-12-31")
    if open_access_only: filters.append("open_access.is_oa:true")
    if doc_type: filters.append(f"type:{'|'.join(doc_type)}")
    if title_only and _ok(q): filters.append(f"title.search:{q.strip()}")
    if filters: params["filter"] = ",".join(filters)

    if sort_opt == "Ø§Ù„Ø£Ø­Ø¯Ø«":            params["sort"] = "publication_date:desc"
    elif sort_opt == "Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ù‹Ø§": params["sort"] = "cited_by_count:desc"
    else:                                params["sort"] = "relevance_score:desc"
    return params

def _request_openalex(url, params, headers):
    resp = requests.get(url, params=params, timeout=30, headers=headers)
    resp.raise_for_status()
    return resp

@st.cache_data(show_spinner=False)
def openalex_page(params, per_page=100, cursor="*"):
    base = CFG.get("openalex_base","https://api.openalex.org/works")
    mailto = (CFG.get("openalex_mailto") or os.environ.get("OPENALEX_MAILTO") or "").strip()
    headers = {"User-Agent": "SearchIt/Pro" + (f" (mailto:{mailto})" if mailto else "")}
    q = dict(params); q["per_page"] = per_page; q["cursor"] = cursor
    if mailto: q["mailto"] = mailto
    data = _request_openalex(base, q, headers).json()
    results = data.get("results", []) or []
    meta = data.get("meta", {}) or {}
    return results, meta.get("next_cursor"), meta.get("count", 0)

# ---------------- ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø« ----------------
results = []
total_count = 0
if do_search and q.strip():
    params = _build_params(q, exact_phrase if 'exact_phrase' in locals() else "", any_words if 'any_words' in locals() else "",
                           author if 'author' in locals() else "", venue if 'venue' in locals() else "",
                           lang_code if 'lang_code' in locals() else "", 
                           years if 'years' in locals() else (1990,2030), 
                           open_access_only if 'open_access_only' in locals() else False, 
                           doc_type if 'doc_type' in locals() else [], 
                           sort_opt if 'sort_opt' in locals() else "Ø§Ù„ØµÙ„Ø© (Ø§ÙØªØ±Ø§Ø¶ÙŠ)",
                           title_only if 'title_only' in locals() else False)
    try:
        results, next_cursor, total_count = openalex_page(params, per_page=100, cursor="*")
    except Exception as e:
        st.error(f"ØªØ¹Ø°Ù‘Ø± Ø§Ù„Ø¬Ù„Ø¨ Ù…Ù† OpenAlex: {e}")
        results, next_cursor, total_count = [], None, 0

    # ÙÙ„ØªØ±Ø© Ù…Ø­Ù„ÙŠØ© Ø°ÙƒÙŠØ© (Ø¨Ø­Ø« Ø¯Ù„Ø§Ù„ÙŠ Ù…Ø­Ù„ÙŠ)
    strict_local_enabled = strict_local if 'strict_local' in locals() else True
    results = _apply_exclude(results, none_words if 'none_words' in locals() else "")
    results = _apply_require(results, q, exact_phrase if 'exact_phrase' in locals() else "", any_words if 'any_words' in locals() else "", enabled=strict_local_enabled)

# ---------------- Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ----------------
def _actions_row(lang, title, pdf, source):
    phone = "201007975534"  # Ø¨Ø¯ÙˆÙ† 00
    # Ø±Ø³Ø§Ù„Ø© Ù…Ø®ØµÙ‘ØµØ© Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ©ØŒ Ù†Ø¶ÙŠÙ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¹Ù…Ù„ Ù„ÙŠØ³Ù‡Ù‘Ù„ Ø¹Ù„Ù‰ Ø§Ù„ÙØ±ÙŠÙ‚ Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø³ÙŠØ§Ù‚
    msg_stats = {
        "ar": f"Ù„Ø¯ÙŠ Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ø¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø¨Ø®ØµÙˆØµ: {title}",
        "en": f"I have a question about research statistics for: {title}",
    }.get(lang, f"I have a question about research statistics for: {title}")
    msg_method = {
        "ar": f"Ù„Ø¯ÙŠ Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„Ø¨Ø­Ø« ÙˆÙ†ØªØ§Ø¦Ø¬Ù‡Ø§ Ø¨Ø®ØµÙˆØµ: {title}",
        "en": f"I have a question about research methodology & results for: {title}",
    }.get(lang, f"I have a question about research methodology & results for: {title}")
    msg_service = {
        "ar": f"Ø£Ø±ØºØ¨ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ Ù„Ø¨Ø­Ø« Ø¨Ø¹Ù†ÙˆØ§Ù†: {title}",
        "en": f"I want a statistical analysis service for the paper: {title}",
    }.get(lang, f"I want a statistical analysis service for the paper: {title}")
    wa_stats = f"https://wa.me/{phone}?text={quote(msg_stats)}"
    wa_method = f"https://wa.me/{phone}?text={quote(msg_method)}"
    wa_service = f"https://wa.me/{phone}?text={quote(msg_service)}"
    wa_general = f"https://wa.me/{phone}?text={quote(title)}"
    parts = []
    if pdf:    parts.append(f'<a href="{pdf}" target="_blank">{t("icon_download_pdf", lang)}</a>')
    if source: parts.append(f'<a href="{source}" target="_blank">{t("icon_download_source", lang)}</a>')
    parts.append(f'<a href="{wa_stats}" target="_blank">{t("icon_stats_q", lang)}</a>')
    parts.append(f'<a href="{wa_method}" target="_blank">{t("icon_method_q", lang)}</a>')
    parts.append(f'<a href="{wa_service}" target="_blank">{t("icon_analysis_service", lang)}</a>')
    parts.append(f'<a href="{wa_general}" target="_blank">{t("icon_whatsapp", lang)}</a>')
    return " | ".join(parts)

if do_search and q.strip():
    st.success(f"{t('total_results', st.session_state.ui_lang)}: {total_count:,}")
    st.caption(f"{t('displayed_after_filter', st.session_state.ui_lang)}: {len(results):,}")
    if not results:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø©. Ø¬Ø±Ù‘Ø¨ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø£Ùˆ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©.")
    else:
        for w in results:
            title = w.get("title") or "â€”"
            venue = _field(w,"host_venue.display_name") or _field(w,"primary_location.source.display_name") or "â€”"
            year  = _field(w,"publication_year") or "â€”"
            authors = ", ".join([_field(a,"author.display_name","") for a in (w.get("authorships") or []) if _field(a,"author.display_name")]) or "â€”"
            pdf   = _field(w, "open_access.oa_url") or _field(w, "primary_location.pdf_url")
            source= _field(w,"primary_location.source.url") or _field(w,"doi") or _field(w,"best_oa_location.url") or _field(w,"id")
            if source and isinstance(source, str) and source.startswith("10."):
                source = f"https://doi.org/{source}"
            cites = int(_field(w,"cited_by_count") or 0)

            # Ø³Ø·Ø± Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§ Ù…ØªØ±Ø¬Ù…
            meta = "  â€¢  ".join([
                f"ğŸ·ï¸ {t('meta_venue', st.session_state.ui_lang)}: **{venue}**",
                f"ğŸ‘¤ {t('meta_authors', st.session_state.ui_lang)}: **{authors}**",
                f"ğŸ“… {t('meta_year', st.session_state.ui_lang)}: **{year}**",
                f"ğŸ“ˆ {t('meta_citations', st.session_state.ui_lang)}: **{cites}**",
            ])

            st.markdown(f"### {title}")
            st.caption(meta)
            st.markdown(_actions_row(st.session_state.ui_lang, title, pdf, source), unsafe_allow_html=True)
            st.markdown("---")
else:
    st.info("Ø§ÙƒØªØ¨ ÙƒÙ„Ù…ØªÙƒ Ø«Ù… Ø§Ø¶ØºØ· Ø²Ø± Ø§Ù„Ø¨Ø­Ø« (ğŸ” Ø§Ø¨Ø­Ø«) Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…. Ø§Ø³ØªØ®Ø¯Ù… \"Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\" Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†ØªØ§Ø¦Ø¬.")
